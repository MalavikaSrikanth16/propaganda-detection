{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_TC_fragments_windows.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmT7zjYngjGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk6leB_Rhgtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQwgnPMLmtDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTxaR7K3iojC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "root_path = \"/content/drive/My Drive/\"\n",
        "train_folder = root_path + \"datasets/train-articles\" # check that the path to the datasets folder is correct,\n",
        "dev_folder = root_path + \"datasets/dev-articles\"     # if not adjust these variables accordingly\n",
        "train_labels_file = root_path + \"datasets/train-task2-TC.labels\"\n",
        "dev_template_labels_file = root_path + \"datasets/dev-task-TC-template.out\"\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import glob\n",
        "import os.path\n",
        "import numpy as np\n",
        "import codecs\n",
        "import pandas as pd\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def read_articles_from_file_list(folder_name, file_pattern=\"*.txt\"):\n",
        "    \"\"\"\n",
        "    Read articles from files matching patterns <file_pattern> from\n",
        "    the directory <folder_name>.\n",
        "    The content of the article is saved in the dictionary whose key\n",
        "    is the id of the article (extracted from the file name).\n",
        "    Each element of <sentence_list> is one line of the article.\n",
        "    \"\"\"\n",
        "    file_list = glob.glob(os.path.join(folder_name, file_pattern))\n",
        "    articles = {}\n",
        "    article_id_list, sentence_id_list, sentence_list = ([], [], [])\n",
        "    for filename in sorted(file_list):\n",
        "        article_id = os.path.basename(filename).split(\".\")[0][7:]\n",
        "        with codecs.open(filename, \"r\", encoding=\"utf8\") as f:\n",
        "            articles[article_id] = f.read().replace(\"\\r\",\"\")\n",
        "    return articles\n",
        "\n",
        "def read_predictions_from_file(filename):\n",
        "    \"\"\"\n",
        "    Reader for the gold file and the template output file.\n",
        "    Return values are four arrays with article ids, labels\n",
        "    (or ? in the case of a template file), begin of a fragment,\n",
        "    end of a fragment.\n",
        "    \"\"\"\n",
        "    articles_id, span_starts, span_ends, gold_labels = ([], [], [], [])\n",
        "    with open(filename, \"r\") as f:\n",
        "        for row in f.readlines():\n",
        "            article_id, gold_label, span_start, span_end = row.rstrip().split(\"\\t\")\n",
        "            articles_id.append(article_id)\n",
        "            gold_labels.append(gold_label)\n",
        "            span_starts.append(span_start)\n",
        "            span_ends.append(span_end)\n",
        "    return articles_id, span_starts, span_ends, gold_labels\n",
        "\n",
        "def get_fragments(articles, article_ids, span_starts, span_ends):\n",
        "    fragments = []\n",
        "    prev_windows = []\n",
        "    next_windows = []\n",
        "    for i in range(len(article_ids)):\n",
        "        idx = article_ids[i]\n",
        "        start = int(span_starts[i])\n",
        "        end = int(span_ends[i])\n",
        "        fragment = articles[idx][start:end]\n",
        "        fragments.append(fragment)\n",
        "        if (start-300) > 0:\n",
        "            s = start-300\n",
        "        else:\n",
        "            s = 0\n",
        "        if (end + 300) <= len(articles[idx]):\n",
        "            e = end + 300\n",
        "        else:\n",
        "            e = len(articles[idx])\n",
        "        next_window_char = articles[idx][end:e]\n",
        "        prev_window_char = articles[idx][s:start]\n",
        "        next_window_tokens = next_window_char.split(\" \")\n",
        "        prev_window_tokens = prev_window_char.split(\" \")\n",
        "        if len(next_window_tokens) > 40:\n",
        "            next_window_tokens = next_window_tokens[0:40]\n",
        "        if len(prev_window_tokens) > 40:\n",
        "            prev_window_tokens = prev_window_tokens[-40:]\n",
        "        next_window = ' '.join(next_window_tokens)\n",
        "        prev_window = ' '.join(prev_window_tokens)\n",
        "        prev_windows.append(prev_window)\n",
        "        next_windows.append(next_window)\n",
        "\n",
        "    return fragments, next_windows, prev_windows\n",
        "\n",
        "def get_sentences_containing_fragments(articles, article_ids, span_starts, span_ends, labels):\n",
        "    d = {}\n",
        "    for i in range(len(article_ids)):\n",
        "        idx = article_ids[i]\n",
        "        fragment = {'start': int(span_starts[i]), 'end' : int(span_ends[i]), 'label' : labels[i]}\n",
        "        if idx not in d:\n",
        "          d[idx] = [fragment]\n",
        "        else:\n",
        "          d[idx].append(fragment)\n",
        "    unique_article_ids = sorted(set(article_ids))\n",
        "    train_sentences = []\n",
        "    train_sent_labels = []\n",
        "    for art_id in unique_article_ids:\n",
        "        article = articles[art_id]\n",
        "        sentences = article.split('\\n')\n",
        "        sentence_indices = []\n",
        "        fragments_in_article = d[art_id]\n",
        "        start_fragment = 0\n",
        "        sent_start = 0\n",
        "        sent_end = 0\n",
        "        for sent in sentences:\n",
        "            sent_start = sent_end\n",
        "            if len(sent) == 1:\n",
        "                sent_end = sent_start + 2\n",
        "                continue\n",
        "            if len(sent) == 0:\n",
        "                sent_end = sent_start + 1\n",
        "                continue\n",
        "            sent_end = sent_start + len(sent) - 1\n",
        "            fragment_in_sent = None\n",
        "            max_length = 0\n",
        "            for frag in fragments_in_article:\n",
        "                if (frag['start'] >= sent_start and frag['start'] < sent_end) or (frag['end'] > sent_start and frag['end'] <= sent_end):\n",
        "                    frag_lt = min(sent_end,frag['end']) - max(sent_start,frag['start'])\n",
        "                    if frag_lt > max_length:\t\n",
        "                        fragment_in_sent = frag\n",
        "                        max_length = frag_lt\n",
        "            if fragment_in_sent != None:\n",
        "\t\t            train_sentences.append(sent) \n",
        "\t\t            train_sent_labels.append(fragment_in_sent['label'])\n",
        "            else:\n",
        "                train_sentences.append(sent)\n",
        "                train_sent_labels.append('No_Propaganda')\n",
        "            sent_end = sent_end + 2        \n",
        "    return train_sentences, train_sent_labels\n",
        "\n",
        "### MAIN ###\n",
        "\n",
        "# loading articles' content from *.txt files in the train folder\n",
        "articles = read_articles_from_file_list(train_folder)\n",
        "\n",
        "# loading gold labels, articles ids and sentence ids from files *.task-TC.labels in the train labels folder\n",
        "ref_articles_id, ref_span_starts, ref_span_ends, train_gold_labels = read_predictions_from_file(train_labels_file)\n",
        "print(\"Loaded %d annotations from %d articles\" % (len(ref_span_starts), len(set(ref_articles_id))))\n",
        "\n",
        "# # compute one feature for each fragment, i.e. the length of the fragment, and train the model\n",
        "train_fragments, train_next_windows, train_prev_windows = get_fragments(articles, ref_articles_id, ref_span_starts, ref_span_ends)\n",
        "train_fragment_labels = train_gold_labels\n",
        "\n",
        "#sentences, sent_labels = get_sentences_containing_fragments(articles, ref_articles_id, ref_span_starts, ref_span_ends, train_gold_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqy-SAmJFJZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Uncomment to write fragments to csv for analysis\n",
        "# def data_for_analysis(fragments, next_windows, prev_windows, labels, article_ids):\n",
        "#     rows = []\n",
        "#     for i in range(len(fragments)):\n",
        "#         l = [labels[i],  prev_windows[i], fragments[i], next_windows[i], article_ids[i]]\n",
        "#         rows.append(l)\n",
        "#     df = pd.DataFrame(rows, columns=['Propaganda Type','Prev window','Fragment','Next window', 'Article ID'])\n",
        "#     df.to_csv(root_path + 'analysis_data.csv')\n",
        "        \n",
        "# data_for_analysis(fragments, next_windows, prev_windows, fragment_labels, ref_articles_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OlPEwQoTVhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create the following datapoints - previous fragment + sentence, sentence + next fragment. These are input to BERT as a sentence pair.\n",
        "train_prev_fragment_list = []\n",
        "train_fragment_next_list = []\n",
        "for i in range(len(train_fragments)):\n",
        "    prev_fragment = train_prev_windows[i] + train_fragments[i]\n",
        "    fragment_next = train_fragments[i] + train_next_windows[i]\n",
        "    train_prev_fragment_list.append(prev_fragment)\n",
        "    train_fragment_next_list.append(fragment_next)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtB8354aed-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs1 = train_prev_fragment_list\n",
        "train_inputs2 = train_fragment_next_list\n",
        "train_labels = train_fragment_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAxB-r59mJL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sentences1 = np.array(train_inputs1)\n",
        "train_sentences2 = np.array(train_inputs2)\n",
        "train_labels, uniques = pd.factorize(train_labels, sort=True)\n",
        "print(uniques)\n",
        "\n",
        "keys = [i for i in range(14)]\n",
        "values = uniques\n",
        "dict_types = dict(zip(keys, values))\n",
        "print(dict_types)\n",
        "\n",
        "labels = np.array(train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dde7bQWboQxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqCI58167-2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
        "train_input_ids = []\n",
        "for s1, s2 in zip(train_sentences1, train_sentences2):\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        s1,                 \n",
        "                        text_pair = s2,\n",
        "                        add_special_tokens = True\n",
        "                   )\n",
        "    train_input_ids.append(encoded_sent)\n",
        "\n",
        "print('Original: ', train_sentences1[0])\n",
        "print('Original 2: ', train_sentences2[0])\n",
        "print('Token IDs:', train_input_ids[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZS6q-uG8d14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in train_input_ids]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk7nOX9T8wmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "MAX_LEN = 520\n",
        "train_input_ids = pad_sequences(train_input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNfkiY009s60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_attention_masks = []\n",
        "for sent in train_input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    train_attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DQzHxfzJI2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_token_type_ids = []\n",
        "for sent in train_input_ids:\n",
        "    for i in range(len(sent)):\n",
        "      if sent[i] == 102:\n",
        "        break\n",
        "    t = [0 for i in range(i+1)]\n",
        "    t =  t + [1] * (len(sent) - len(t))\n",
        "    train_token_type_ids.append(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Taael7UNBwom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs = train_input_ids\n",
        "train_masks = train_attention_masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQndN7yFBggD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "train_token_type_ids = torch.tensor(train_token_type_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95mdJ3GsFcsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "train_batch_size = 8\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_token_type_ids, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kucdGvJGKdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", \n",
        "    num_labels = 14, \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn_bSvsoGvz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_articles = read_articles_from_file_list(dev_folder)\n",
        "dev_article_ids, dev_span_starts, dev_span_ends, dev_labels = read_predictions_from_file(dev_template_labels_file)\n",
        "dev_fragments, dev_next_windows, dev_prev_windows = get_fragments(dev_articles, dev_article_ids, dev_span_starts, dev_span_ends)\n",
        "#dev_sentences, dev_labels = get_sentences_containing_fragments(dev_articles, dev_article_ids, dev_span_starts, dev_span_ends, dev_labels)\n",
        "\n",
        "dev_prev_fragment_list = []\n",
        "dev_fragment_next_list = []\n",
        "for i in range(len(dev_fragments)):\n",
        "    prev_fragment = dev_prev_windows[i] + dev_fragments[i]\n",
        "    fragment_next = dev_fragments[i] + dev_next_windows[i]\n",
        "    dev_prev_fragment_list.append(prev_fragment)\n",
        "    dev_fragment_next_list.append(fragment_next)\n",
        "\n",
        "dev_inputs1 = dev_prev_fragment_list\n",
        "dev_inputs2 = dev_fragment_next_list\n",
        "\n",
        "dev_sentences1 = np.array(dev_inputs1)\n",
        "dev_sentences2 = np.array(dev_inputs2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYcCPRkUGOX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_input_ids = []\n",
        "for s1,s2 in zip(dev_sentences1, dev_sentences2):\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        s1,                 \n",
        "                        text_pair = s2,\n",
        "                        add_special_tokens = True\n",
        "                   )\n",
        "    dev_input_ids.append(encoded_sent)\n",
        "\n",
        "dev_input_ids = pad_sequences(dev_input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "dev_attention_masks = []\n",
        "for seq in dev_input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  dev_attention_masks.append(seq_mask) \n",
        "\n",
        "dev_token_type_ids = []\n",
        "for sent in dev_input_ids:\n",
        "    for i in range(len(sent)):\n",
        "      if sent[i] == 102:\n",
        "        break\n",
        "    t = [0 for i in range(i+1)]\n",
        "    t =  t + [1] * (len(sent) - len(t))\n",
        "    dev_token_type_ids.append(t)\n",
        "\n",
        "prediction_inputs = torch.tensor(dev_input_ids)\n",
        "prediction_masks = torch.tensor(dev_attention_masks)\n",
        "prediction_token_type_ids = torch.tensor(dev_token_type_ids)\n",
        "\n",
        "dev_batch_size = 8\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_token_type_ids)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=dev_batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVoc6tOGG0gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5,\n",
        "                  eps = 1e-8 \n",
        "                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToTaqR2SIjud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "epochs = 6\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RMLgWxoKhMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h90RWSA8K7N4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QG5e7fGK_v-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "loss_values = []\n",
        "for epoch_i in range(0, epochs):\n",
        "    print(\"\")\n",
        "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
        "    print('Training')\n",
        "    t0 = time.time()\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_token_type_ids = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "        model.zero_grad()\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=b_token_type_ids, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uHjf0g9iVRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_token_type_ids = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids, token_type_ids=b_token_type_ids, \n",
        "                  attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    predictions = predictions + list(logits)\n",
        "\n",
        "predicted_labels = []\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "    pred_labels_i = np.argmax(predictions[i])\n",
        "    predicted_labels.append(pred_labels_i)\n",
        "\n",
        "predicted_labels = [dict_types[pred] for pred in predicted_labels]\n",
        "\n",
        "# writing predictions to file to be uploaded to semeval task 11 submissions page.\n",
        "task_TC_output_file = root_path + \"output-TC-bert-fragprevnext-epoch-\" + str(epoch_i) + \".txt\"\n",
        "\n",
        "with open(task_TC_output_file, \"w\") as fout:\n",
        "    for article_id, prediction, span_start, span_end in zip(dev_article_ids, predicted_labels, dev_span_starts, dev_span_ends):\n",
        "        fout.write(\"%s\\t%s\\t%s\\t%s\\n\" % (article_id, prediction, span_start, span_end))\n",
        "print(\"Predictions written to file \" + task_TC_output_file) \n",
        "print(\"Done predicting for epoch\" + str(epoch_i))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}